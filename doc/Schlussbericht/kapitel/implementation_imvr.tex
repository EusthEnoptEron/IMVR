\chapter{Implementation von IMVR}

Der Indexer ist implementiert und dokumentiert, also fehlt nur noch die eigentliche Applikation. Vieles hat sich im Laufe des Projekts verändert und entwickelt, und viele Probleme traten zum Vorschein, die an dieser Stelle genauer erläutert werden sollen.

\section{Unity 5}

Die Implementation von IMVR lässt sich leider nicht ohne einen gewissen Hintergrund in Unity 5 erklären. 

\subsection{Grundkonzepte}

Unity ist eine Entwicklungsumgebung und eine Spiel-Engine, die momentan aufgrund ihrer Bedienungsfreundlichkeit  und einer  frei erhältlichen  Version sehr  beliebt in  der Szene  der Indie-Developer ist. Gleichzeitig dient sie auch als gutes Prototyping-Tool, um schnell Ideen umzusetzen. 

Um Unity in groben Zügen zu erklären, sollen zwei Ansichtspunkte beschrieben werden: der Szenenaufbau und die Ressourcenverwaltung. 

Ein  Projekt  in  Unity  ist  in  sogenannte  \textit{Scenes}  (Szenen)  gegliedert,  welche  aus  Objekten (\textit{GameObject})  bestehen  –  das  Grundprinzip  der  Scene-Graphs  wird  also  angewandt. Speziell  ist,  dass  die  Interaktion  und  Spiellogik  grundsätzlich  nur  innerhalb  von \textit{Components}  geschieht.  Jedes  Script  und  jede  \textit{Eigenschaft}  wird  als  Component  einem GameObject  zugeordnet.  So  hat  zum  Beispiel  ein  Licht  ein  \textit{Light}-Component  oder  die Kamera ein \textit{Camera}-Component. Selbst die Position jedes GameObjects ist nur ein Wert im  \textit{Transform}-Component.

Ein  anderer  Aspekt  von  Unity  ist  die  Ressourcenverwaltung.  Im  Grunde  genommen  ist  es 
dem  Programmierer  überlassen,  wie  er  seine  Ressourcen  verwaltet.  Das  einzige,  was 
beachtet  werden  muss,  ist,  dass  alle  Ressourcen  im  \textit{Assets}-Folder  abgelegt  werden 
müssen.  Üblicherweise  wird  dann  ein  Ordner  für  jede  Art  von  Asset  erstellt,  z.B.  für 
Materialien, Texturen, Meshes, Scripts, etc.

Ein  weiterer  wichtiger  Begriff  sind  die  \textit{Prefabs}.  Damit  sind  Vorlagen  gemeint,  die  man 
erstellt, indem man fertige Objekte aus der aktuellen Szene in das Asset-Folder zieht, und 
danach wiederverwerten kann. 

\subsection{Unity im Kontext von IMVR}

Was sofort auffällt bei der Betrachtung der Ressourcenverwaltung, ist, dass diese starke Kopplung von Assets zu Projekten sich mit der grundlegenden Aufgabe dieses Projektes beisst. Unity sieht vor, dass der Programmierer während der Entwicklung alle seine Assets in den dafür vorgesehenen Ordner platziert, das Projekt am Schluss kompiliert, und dann höchstens im Nachhinein neue Assets als \textit{Asset Bundles} an seine Anwender verteilt. In diesem Projekt ist es jedoch zwingend nötig, dynamisch Bilder und Musik anhand der Dateien auf dem Anwender-PC zu laden. Auf die Folgen und Lösungen zu diesem Problem wird in den Kapiteln \ref{subsec:resources} und  \ref{subsec:musicformats} näher eingegangen.


\section{Aufbau}

\section{Interaktionskonzept}

Unüberraschenderweise findet die Interaktion des Users mit IMVR fast ausschliesslich mit seinen Händen statt. In einer frühen Phase des Projektes war noch geplant, eventuell die Spracheingabe modal zu den Händen zu gebrauchen, jedoch reichte dafür die Zeit nicht mehr. Ein Artefakt dieses Vorhabens ist das \textit{SpeechServer}-Projekt, welches sich immer noch unter den \textit{AuxiliaryTools} befindet.


\subsection{Ringmenü}

Bei der Entwicklung von VR-Applikationen stösst man zwingenderweise auf Situationen, in denen herkömmliche Konzepte nicht mehr verwendet werden können. Die Platzierung und der Aufbau des Menüs ist so ein Punkt.

Es ist nicht leicht ein Menü korrekt zu platzieren. Eine statische Platzierung als Overlay hält das Interface zwar im sichtbaren Bereich, kann sich jedoch als "lästig" herausstellen. Lässt man es verzögert mitschweben, gerät das Menü sofort ausser Kontrolle, und stellt man es irgendwo in die Szene und belässt es dabei, verliert man es sofort aus dem Blick.

Im Falle von IMVR bieten sich jedoch die Hände als gut verwendbarer Ankerpunkt für das Menü an. Es gibt ein freies Projekt auf GitHub \cite{hoverVR}, welches die Finger der Hand für die Platzierung der Buttons in einer ringartigen Struktur verwendet. Leider befand sich das Projekt in einem zu instabilen Stadium für diese Arbeit, aber es lieferte die Idee für eine eigene, ähnliche Implementierung.

Für IMVR wurde ebenfalls ein Ringmenü entwickelt, doch dieses verfügt über keine Schaltflächen im herkömmlichen Sinn. Die Finger werden auch mit Funktionen versehen, aber zum Betätigen benutzt der Anwender nicht seine andere Hand, sondern hebt einen Finger. Wenn ein Finger lange genug gehoben wird, wird die zugewiesene Aktion ausgeführt.

\subsection{Fussplatten}

Beim Erstellen einer visuellen Applikation stellt sich die Frage, wie man am besten die Struktur verdeutlichen kann. Eine Technik, die dafür gewählt wurde, ist der Einsatz von "Fussplatten".

Hierbei befinden sich unter den Füssen des Anwenders ringförmige Platten, welche die zwei Modi der Applikation repräsentieren. Eine dritte, zentriert abgehobene Platte dient zur Beendigung der Applikation.

Bei diesen Platten handelt es sich um das einzige Interaktionsmittel, welches bewusst keine Eingabe durch die Hände erfordert. In diesem Fall wird die Oculus Rift selbst als Eingabegerät verwendet, und zwar durch den Blickwinkel.

Die Idee ist, dass der Benutzer feststellen will, ``wo er steht'', herunterschaut, und durch gehaltenen Blickkontakt mit den Fussplatten diese aktivieren kann. Entsprechend den \textit{Best Practices} wird dabei ein Indikator gefüllt, der anzeigt, wie lange der Blick noch gehalten werden muss.

Diese Art von Eingabe lässt sich oft bei bereits erschienen Demos für die Oculus Rift beobachten. Das Prinzip ist sehr leicht zu implementieren und daher auch verlockend, jedoch muss mit Vorsicht vorgegangen werden: Für den Benutzer ist es auf Dauer unangenehm, wenn von ihm ständige Kopfbewegungen gefordert werden. In IMVR wurde jedoch bewusst Gebrauch von dieser Methode gemacht, weil der Anwender nur selten nach unten schauen wird, und es relativ intuitiv ist.

\subsection{Music Arm}

Da es in dieser Arbeit voll und ganz um das Eingreifen ins Geschehen mit den eigenen Händen geht, wäre es bedauerlich, wenn es nicht möglich wäre, die Musik mit den Händen zu steuern. Ein weiteres Konzept, welches genau dieses Problem löst, ist der sogenannte "Music Arm".

Beim Music Arm handelt es sich um das Interface, welches an Stelle des menschlichen Arms angezeigt wird. Dieser digitale Arm bietet die Möglichkeit, die Playlist zu verwalten, sowie die momentan abgespielte Musik anzuschauen bzw. vor- und zurückzuspulen.

Hierbei wird eine Technik verwendet, welche einen Unterschied zwischen der Vorder- und der Rückseite macht. Schaut der Benutzer auf die ``Uhr'', also hält seinen Handrücken vor sich, dann erscheint ein Interface, in welchem er alle wichtigen Informationen über das momentane Lied erhält (Name, Artist, Cover, Länge und Fortschritt).


\begin{figure}[h]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{bilder/musicarm_front}
		\caption{Vorderseitenmenü}
		\label{fig:musicarm_front}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.5\linewidth]{bilder/musicarm_back}
		\caption{Rückseitenmenü}
		\label{fig:musicarm_back}
	\end{subfigure}
	\caption{Darstellung des Music Arms}
	\label{fig:musicarm_frontback}
\end{figure}


Dreht der Anwendet jedoch seinen Arm und schaut auf die Rückseite, wird er eine Liste erhalten, die er scrollen und  selektieren kann. Bei der Selektion wird das Element auf das Ringmenü "gesendet", wo der Anwender dann die Möglichkeit hat, das Lied abzuspielen, oder die Selektion aufzuheben. Würde man sofort bei der Selektion ein Lied abspielen, ergäbe sich das Problem, dass durch die mangelhafte Genauigkeit der Leap Motion und der Bedienung allgemein viele Fehlselektionen passieren würden und somit diverse Lieder zufällig abgespielt würden. Abbildung \ref{fig:musicarm_frontback} zeigt, wie dieses Interface aussieht.

Der Entscheid, welche Seite angezeigt wird, geschieht über ein simples Skalarprodukt. Der Forward-Vektor der Kamera wird verglichen mit der Normale des Arms, welche in die gleiche Richtung zeigt, wie der Handrücken. Wenn der Wert tiefer als -0.5 ist, die Vektoren also entgegengerichtet sind, wird die Unterseite erkannt, und wenn der Wert höher als 0.5 ist, die Oberseite. Der Entscheid könnte auch bei 0 gefällt werden, aber dann würden auch Grenzfälle angezeigt werden, in denen besser gar kein Menü angezeigt wird.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\linewidth]{diagramme/musicarm}
\caption{Bestimmung der momentanen Armseite}
\label{fig:musicarm}
\end{figure}

\subsection{Slider Balken}

Im Hörteil der Applikation, dem blauen Teil, sollte es dem Anwender ermöglicht werden, die Features der gewünscheten Musik anzugeben, und diese dann anzuhören. Wie bereits erwähnt, bilden diese Werte einen Bereich zwischen 0 und 1 ab (mit Ausnahme des Tempos). Will der Anwender eine energievolle Musik hören, muss es ihm deshalb möglich sein, neben dem Maximalwert auch einen Minimalwert anzugeben. Es liegt also ein typischer Use-Case für einen Range-Slider vor.

Da sich kein solcher im Werkzeugkasten von Unity befindet, musste ein eigener erstellt werden. Um einen anderen Approach zu testen, als mit der starken Verwendung von Unitys neuem UI-System in den anderen Teilen der Applikation gewählt wurde, wurde in diesem Fall bewusst ein Slider mithilfe eines Zylinder-Meshes erstellt.

Beim implementierten Slider hat der Anwender die Möglichkeit, direkt mit seinen zwei Händen einen Auswahlbereich einzustellen, indem er diese parallel in den Zylinder hält und entsprechend bewegt. Anders als die relativ abstrakte Implementation der anderen Interaktionselemente, basiert dieser Slider auf das Kollisionsmodell von Unity und die physikalische Hände, die mit dem Leap Motion Plugin mitgeliefert werden.

Das System funktioniert so, dass zu Beginn eine Achse $\hat{v}_{richtung}$ festgelegt wird, in die sich der Zylinder füllt (z.B. von links nach rechts). Bei einer Kollision der Hand mit dem Zylinder, wird nun zuerst geprüft, ob die Hand gültig ist. Sofern das der Fall ist, wird die Position der Handfläche $\vec{P}_{hand}$ auf die Achse $\hat{v}_{richtung}$ projiziert und auf die Länge des Zylinders skaliert.

Der Wert, der dadurch entsteht, wird dann je nach geprüfter Handseite als neues Minimum bzw. Maximum verwendet. Allerdings geschieht davor noch eine lineare Interpolation vom vorigen Wert für einen weichen Übergang und ein Snapping zum Vereinfachen der Auswahl des Maximums bzw. Minimums.

Damit andere Teile der Applikation auf diese Wertänderungen reagieren können, wird beim Ändern der Extrema jeweils die Events \code{onMinValueChanged} bzw. \code{onMaxValueChanged} sowie \code{onValueChanged} ausgelöst. Durch Abfangen dieser Events wird z.B. der Zähler im \textit{Selector} bei Änderungen der Selektion aktualisiert.

%\begin{algorithm}
%	\caption{My algorithm}
%	\begin{algorithmic}[1]
%		\Procedure{MyProcedure}{}
%		\State $H \gets Hand$
%		\State $\vec{P}_{hand} \gets \text{Position der Hand}$
%		\State $\vec{P}_{min} \gets \text{Position wenn leer}$
%		\State $\vec{P}_{max} \gets \text{Position wenn voll}$
%		\State $\hat{v}_{dir} \gets \frac{\vec{P}_{max}-\vec{P}_{min}}{|\vec{P}_{max}-\vec{P}_{min}|}$
%		\If {$\text{isValid(}\textit{H}\text{)}$}
%		\State $j \gets j-1$.
%		\State $i \gets i-1$.
%		\State \textbf{goto} \emph{loop}.
%		\State \textbf{close};
%		\EndIf
%		\State $i \gets i+\max(\textit{delta}_1(\textit{string}(i)),\textit{delta}_2(j))$.
%		\State \textbf{goto} \emph{top}.
%		\EndProcedure
%	\end{algorithmic}
%\end{algorithm}


\section{Visual Design}

\subsection{Darstellung der Hände}

\subsection{Point Chart}

\subsection{Farben uns Symbolik}

\subsection{Beat Detector und Wellen}

\subsection{Visualisierung der Musik}

\subsection{Globus}


\section{Herausforderungen}

\subsection{Ressourcen-Management}
\label{subsec:resources}

\subsection{Abspielen externer Musik}
\label{subsec:musicformats}

\subsection{Canvas-Elemente}

% Alpha-Sorting, etc. Tile-Blanket