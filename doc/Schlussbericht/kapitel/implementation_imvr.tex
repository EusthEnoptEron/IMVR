\chapter{Implementation von IMVR}
\label{chap:implementation_imvr}

Der Indexer ist implementiert und dokumentiert, also fehlt nur noch die eigentliche Applikation. Vieles hat sich im Laufe des Projekts verändert und entwickelt, und viele Probleme traten zum Vorschein, die an dieser Stelle genauer erläutert werden sollen.

\section{Unity 5}

Die Implementation von IMVR lässt sich leider nicht ohne einen gewissen Hintergrund in Unity 5 erklären. 

\subsection{Grundkonzepte}

Unity ist eine Entwicklungsumgebung und eine Spiel-Engine, die momentan aufgrund ihrer Bedienungsfreundlichkeit  und einer  frei erhältlichen  Version sehr  beliebt in  der Szene  der Indie-Developer ist. Gleichzeitig dient sie auch als gutes Prototyping-Tool, um schnell Ideen umzusetzen. 

Um Unity in groben Zügen zu erklären, sollen zwei Ansichtspunkte beschrieben werden: der Szenenaufbau und die Ressourcenverwaltung. 

Ein  Projekt  in  Unity  ist  in  sogenannte  \textit{Scenes}  (Szenen)  gegliedert,  welche  aus  Objekten (\textit{GameObject})  bestehen  –  das  Grundprinzip  der  Scene-Graphs  wird  also  angewandt. Speziell  ist,  dass  die  Interaktion  und  Spiellogik  grundsätzlich  nur  innerhalb  von \textit{Components}  geschieht.  Jedes  Script  und  jede  \textit{Eigenschaft}  wird  als  Component  einem GameObject  zugeordnet.  So  hat  zum  Beispiel  ein  Licht  ein  \textit{Light}-Component  oder  die Kamera ein \textit{Camera}-Component. Selbst die Position jedes GameObjects ist nur ein Wert im  \textit{Transform}-Component.

Ein  anderer  Aspekt  von  Unity  ist  die  Ressourcenverwaltung.  Im  Grunde  genommen  ist  es 
dem  Programmierer  überlassen,  wie  er  seine  Ressourcen  verwaltet.  Das  einzige,  was 
beachtet  werden  muss,  ist,  dass  alle  Ressourcen  im  \textit{Assets}-Folder  abgelegt  werden 
müssen.  Üblicherweise  wird  dann  ein  Ordner  für  jede  Art  von  Asset  erstellt,  z.B.  für 
Materialien, Texturen, Meshes, Scripts, etc.

Ein  weiterer  wichtiger  Begriff  sind  die  \textit{Prefabs}.  Damit  sind  Vorlagen  gemeint,  die  man 
erstellt, indem man fertige Objekte aus der aktuellen Szene in das Asset-Folder zieht, und 
danach wiederverwerten kann. 

\subsection{Unity im Kontext von IMVR}

Was sofort auffällt bei der Betrachtung der Ressourcenverwaltung, ist, dass diese starke Kopplung von Assets zu Projekten sich mit der grundlegenden Aufgabe dieses Projektes beisst. Unity sieht vor, dass der Programmierer während der Entwicklung alle seine Assets in den dafür vorgesehenen Ordner platziert, das Projekt am Schluss kompiliert, und dann höchstens im Nachhinein neue Assets als \textit{Asset Bundles} an seine Anwender verteilt. In diesem Projekt ist es jedoch zwingend nötig, dynamisch Bilder und Musik anhand der Dateien auf dem Anwender-PC zu laden. Auf die Folgen und Lösungen zu diesem Problem wird in den Kapiteln \ref{subsec:resources} und  \ref{subsec:musicformats} näher eingegangen.


\section{Aufbau}

\section{Interaktionskonzept}

Unüberraschenderweise findet die Interaktion des Users mit IMVR fast ausschliesslich mit seinen Händen statt. In einer frühen Phase des Projektes war noch geplant, eventuell die Spracheingabe modal zu den Händen zu gebrauchen, jedoch reichte dafür die Zeit nicht mehr. Ein Artefakt dieses Vorhabens ist das \textit{SpeechServer}-Projekt, welches sich immer noch unter den \textit{AuxiliaryTools} befindet.


\subsection{Ringmenü}

Bei der Entwicklung von VR-Applikationen stösst man zwingenderweise auf Situationen, in denen herkömmliche Konzepte nicht mehr verwendet werden können. Die Platzierung und der Aufbau des Menüs ist so ein Punkt.

Es ist nicht leicht ein Menü korrekt zu platzieren. Eine statische Platzierung als Overlay hält das Interface zwar im sichtbaren Bereich, kann sich jedoch als "lästig" herausstellen. Lässt man es verzögert mitschweben, gerät das Menü sofort ausser Kontrolle, und stellt man es irgendwo in die Szene und belässt es dabei, verliert man es sofort aus dem Blick.

Im Falle von IMVR bieten sich jedoch die Hände als gut verwendbarer Ankerpunkt für das Menü an. Es gibt ein freies Projekt auf GitHub \cite{hoverVR}, welches die Finger der Hand für die Platzierung der Buttons in einer ringartigen Struktur verwendet. Leider befand sich das Projekt in einem zu instabilen Stadium für diese Arbeit, aber es lieferte die Idee für eine eigene, ähnliche Implementierung.

Für IMVR wurde ebenfalls ein Ringmenü entwickelt, doch dieses verfügt über keine Schaltflächen im herkömmlichen Sinn. Die Finger werden auch mit Funktionen versehen, aber zum Betätigen benutzt der Anwender nicht seine andere Hand, sondern hebt einen Finger. Wenn ein Finger lange genug gehoben wird, wird die zugewiesene Aktion ausgeführt.

\subsection{Fussplatten}

Beim Erstellen einer visuellen Applikation stellt sich die Frage, wie man am besten die Struktur verdeutlichen kann. Eine Technik, die dafür gewählt wurde, ist der Einsatz von "Fussplatten".

Hierbei befinden sich unter den Füssen des Anwenders ringförmige Platten, welche die zwei Modi der Applikation repräsentieren. Eine dritte, zentriert abgehobene Platte dient zur Beendigung der Applikation.

Bei diesen Platten handelt es sich um das einzige Interaktionsmittel, welches bewusst keine Eingabe durch die Hände erfordert. In diesem Fall wird die Oculus Rift selbst als Eingabegerät verwendet, und zwar durch den Blickwinkel.

Die Idee ist, dass der Benutzer feststellen will, ``wo er steht'', herunterschaut, und durch gehaltenen Blickkontakt mit den Fussplatten diese aktivieren kann. Entsprechend den \textit{Best Practices} wird dabei ein Indikator gefüllt, der anzeigt, wie lange der Blick noch gehalten werden muss.

Diese Art von Eingabe lässt sich oft bei bereits erschienen Demos für die Oculus Rift beobachten. Das Prinzip ist sehr leicht zu implementieren und daher auch verlockend, jedoch muss mit Vorsicht vorgegangen werden: Für den Benutzer ist es auf Dauer unangenehm, wenn von ihm ständige Kopfbewegungen gefordert werden. In IMVR wurde jedoch bewusst Gebrauch von dieser Methode gemacht, weil der Anwender nur selten nach unten schauen wird, und es relativ intuitiv ist.

%TODO: note that ring mesh can be seen in \ref{subsec:waves}

\subsection{Music Arm}

Da es in dieser Arbeit voll und ganz um das Eingreifen ins Geschehen mit den eigenen Händen geht, wäre es bedauerlich, wenn es nicht möglich wäre, die Musik mit den Händen zu steuern. Ein weiteres Konzept, welches genau dieses Problem löst, ist der sogenannte "Music Arm".

Beim Music Arm handelt es sich um das Interface, welches an Stelle des menschlichen Arms angezeigt wird. Dieser digitale Arm bietet die Möglichkeit, die Playlist zu verwalten, sowie die momentan abgespielte Musik anzuschauen bzw. vor- und zurückzuspulen.

Hierbei wird eine Technik verwendet, welche einen Unterschied zwischen der Vorder- und der Rückseite macht. Schaut der Benutzer auf die ``Uhr'', also hält seinen Handrücken vor sich, dann erscheint ein Interface, in welchem er alle wichtigen Informationen über das momentane Lied erhält (Name, Artist, Cover, Länge und Fortschritt).


\begin{figure}[h]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{bilder/musicarm_front}
		\caption{Vorderseitenmenü}
		\label{fig:musicarm_front}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.5\linewidth]{bilder/musicarm_back}
		\caption{Rückseitenmenü}
		\label{fig:musicarm_back}
	\end{subfigure}
	\caption{Darstellung des Music Arms}
	\label{fig:musicarm_frontback}
\end{figure}


Dreht der Anwendet jedoch seinen Arm und schaut auf die Rückseite, wird er eine Liste erhalten, die er scrollen und  selektieren kann. Bei der Selektion wird das Element auf das Ringmenü "gesendet", wo der Anwender dann die Möglichkeit hat, das Lied abzuspielen, oder die Selektion aufzuheben. Würde man sofort bei der Selektion ein Lied abspielen, ergäbe sich das Problem, dass durch die mangelhafte Genauigkeit der Leap Motion und der Bedienung allgemein viele Fehlselektionen passieren würden und somit diverse Lieder zufällig abgespielt würden. Abbildung \ref{fig:musicarm_frontback} zeigt, wie dieses Interface aussieht.

Der Entscheid, welche Seite angezeigt wird, geschieht über ein simples Skalarprodukt. Der Forward-Vektor der Kamera wird verglichen mit der Normale des Arms, welche in die gleiche Richtung zeigt, wie der Handrücken. Wenn der Wert tiefer als -0.5 ist, die Vektoren also entgegengerichtet sind, wird die Unterseite erkannt, und wenn der Wert höher als 0.5 ist, die Oberseite. Der Entscheid könnte auch bei 0 gefällt werden, aber dann würden auch Grenzfälle angezeigt werden, in denen besser gar kein Menü angezeigt wird.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\linewidth]{diagramme/musicarm}
\caption{Bestimmung der momentanen Armseite}
\label{fig:musicarm}
\end{figure}

\subsection{Slider Balken}

Im Hörteil der Applikation, dem blauen Teil, sollte es dem Anwender ermöglicht werden, die Features der gewünscheten Musik anzugeben, und diese dann anzuhören. Wie bereits erwähnt, bilden diese Werte einen Bereich zwischen 0 und 1 ab (mit Ausnahme des Tempos). Will der Anwender eine energievolle Musik hören, muss es ihm deshalb möglich sein, neben dem Maximalwert auch einen Minimalwert anzugeben. Es liegt also ein typischer Use-Case für einen Range-Slider vor.

Da sich kein solcher im Werkzeugkasten von Unity befindet, musste ein eigener erstellt werden. Um einen anderen Approach zu testen, als mit der starken Verwendung von Unitys neuem UI-System in den anderen Teilen der Applikation gewählt wurde, wurde in diesem Fall bewusst ein Slider mithilfe eines Zylinder-Meshes erstellt.

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{bilder/cylinder-slider}
	\caption{In-Game Screenshot zur Bedienung der Slider-Balken}
	\label{fig:cylinder-slider}
\end{figure}


Beim implementierten Slider hat der Anwender die Möglichkeit, direkt mit seinen zwei Händen einen Auswahlbereich einzustellen, indem er diese parallel in den Zylinder hält und entsprechend bewegt. Anders als die relativ abstrakte Implementation der anderen Interaktionselemente, basiert dieser Slider auf das Kollisionsmodell von Unity und die physikalische Hände, die mit dem Leap Motion Plugin mitgeliefert werden.

Das System funktioniert so, dass zu Beginn eine Achse $\hat{v}_\textit{dir}$ festgelegt wird, in die sich der Zylinder füllt (z.B. von links nach rechts). Bei einer Kollision der Hand mit dem Zylinder, wird nun zuerst geprüft, ob die Hand gültig ist. Sofern das der Fall ist, wird die Position der Handfläche $\vec{P}_\textit{hand}$ auf die Achse $\hat{v}_\textit{dir}$ projiziert und auf die Länge des Zylinders skaliert.

Der Wert, der dadurch entsteht, wird dann je nach geprüfter Handseite als neues Minimum bzw. Maximum verwendet. Allerdings geschieht davor noch eine lineare Interpolation vom vorigen Wert für einen weichen Übergang und ein Snapping zum Vereinfachen der Auswahl des Maximums bzw. Minimums.

Damit andere Teile der Applikation auf diese Wertänderungen reagieren können, wird beim Ändern der Extrema jeweils die Events \code{onMinValueChanged} bzw. \code{onMaxValueChanged} sowie \code{onValueChanged} ausgelöst. Durch Abfangen dieser Events wird z.B. der Zähler im \textit{Selector} bei Änderungen der Selektion aktualisiert.

%\begin{algorithm}
%	\caption{My algorithm}
%	\begin{algorithmic}[1]
%		\Procedure{MyProcedure}{}
%		\State $H \gets Hand$
%		\State $\vec{P}_{hand} \gets \text{Position der Hand}$
%		\State $\vec{P}_{min} \gets \text{Position wenn leer}$
%		\State $\vec{P}_{max} \gets \text{Position wenn voll}$
%		\State $\hat{v}_{dir} \gets \frac{\vec{P}_{max}-\vec{P}_{min}}{|\vec{P}_{max}-\vec{P}_{min}|}$
%		\If {$\text{isValid(}\textit{H}\text{)}$}
%		\State $j \gets j-1$.
%		\State $i \gets i-1$.
%		\State \textbf{goto} \emph{loop}.
%		\State \textbf{close};
%		\EndIf
%		\State $i \gets i+\max(\textit{delta}_1(\textit{string}(i)),\textit{delta}_2(j))$.
%		\State \textbf{goto} \emph{top}.
%		\EndProcedure
%	\end{algorithmic}
%\end{algorithm}


\section{Visual Design}

Viele Elemente in IMVR haben mit dem Visuellen zu tun. Sie sollen die Musik unterstützend begleiten, Verhältnisse darstellen und natürlich gefallen.

\subsection{Darstellung der Hände}

Das erste Thema, das beim Visuellen einfällt, sind die Hände. Bei der Gestaltung der Hände waren grundsätzlich folgende Voraussetzungen zu erfüllen:

\begin{enumerate}
	\item handförmigs
	\item abstrakt
	\item momentaner Modus ist erkennbar
\end{enumerate}

Der erste Punkt versteht sich von selbst. Der zweite Punkt, die Abstraktheit, kommt daher, weil es am besten zur Applikation passt. Eine echt-aussehende, materielle Hand würde fehl am Platz wirken, wenn der Rest der Szene fast ausschliesslich aus Geometrie besteht und keinen Zusammenhang mit der Realität hat.

Neben dem erwähnten Argument, hat eine abstrakte Hand auch den Vorteil, dass die Darstellung der momentan Musik angepasst werden kann, z.B. durch Farbe oder Bewegung. % TODO: Ist sowas drin? 

Beim dritten Punkt ist mit \textit{Modus} der aktuelle Applikationsmodus gemeint, also der Browse- bzw. Listen-Modus. Diese werden unter Anderem durch die Farbe unterschieden, und diese Farbe zeigt sich auch bei der Visualisierung der Hände.

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{bilder/hand1}
	\caption{Die ursprüngliche Visualisierung der Hand}
	\label{fig:hand1}
\end{figure}

Für die erste Art der Visualisierung (Abbildung \ref{fig:hand1}) wurde eine leicht abgeänderte Form einer Standardhand im Leap Motion Package gewählt. Dabei handelte es sich um eine auf Voxel basierende Darstellung. Für jeden Knochen in der Hand wurde ein Voxel-Sheet erstellt, welches dann den Bewegungen der Hand gefolgt ist. In der Implementation in IMVR wurden lediglich die Voxels mit Kugeln ersetzt, um ein bisschen Individualität zu schaffen.

Diese Darstellungsform wurde jedoch im Laufe des Projekts zugunsten einer partikelbasierten Implementation verworfen. In der ersten partikelbasierten Form, wurde versucht eine geriggte Hand mit Partikeln zu umrahmen statt das Mesh selbst zu rendern. Das war jedoch mit ein paar grossen Nachteilen verbunden:

\begin{enumerate}[label=(\alph*)]
	\item Die Bone-Weights werden auf der GPU appliziert, deshalb muss das Mesh in jedem Frame auf der CPU nachgebildet werden.
	\item Durch die hohe Zahl der Vertices werden über 10\,000 Partikel pro Hand erstellt, was sich sehr schlecht auf die Performance auswirkt.
	\item Da das Finden von Punkten innerhalb eines komplizierten Meshes nicht trivial ist, müssen alle Punkte auf der Hülle platziert werden.  
\end{enumerate}

Aufgrund dieser Nachteile wurde die Implementation schliesslich noch einen Schritt weiter abgeändert. In der finalen Visualisierung (Abbildung \ref{fig:hand2}) wird eine abstrakte Hand, welche aus mehreren Meshes besteht, mit Partikeln \textit{gefüllt}. Damit erhält der Anwender ein intuitives Feedback, wenn die Hand erkannt wird, anstatt dass diese aus dem Nichts erscheint. Damit er trotzdem nicht auf den Effekt warten muss, erhält die Hand einen halb-durchsichtigen Rahmen.

\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{bilder/hand2}
	\caption{Die finale Visualisierung der Hand mit Partikeln}
	\label{fig:hand2}
\end{figure}

Ein Grossteil des Codes konnte von der vorhergehenden Implementation mit dem geriggten Modell übernommen werden. Der Vorteil hier ist, dass die Hand auf \textit{Transforms} aufbaut: Die Submeshes, ein Mesh pro Knochen, werden durch ihre \textit{Transforms} korrekt platziert, damit eine Hand gebildet wird. Dadurch entsteht zwar nicht ein fliessender Übergang wie bei einer geriggten Hand, aber man kann mit primitiven Elementen arbeiten.

In diesem Fall wurde ein Handmodell gewählt, welches grösstenteils aus einfachen, abgerundeten Quadern besteht. Bei der Erstellung der Hand, werden alle Meshes abgelaufen und für jedes Mesh eine gewisse Anzahl Partikel erstellt. Damit diese \textit{innerhalb} der Hand erscheinen, und nicht auf der Hülle wie bei der vorherigen Implementation, wird jede Partikelposition aus zwei zufälligen, verschiedenen Vertices gebildet mit einer ebenfalls zufälligen Gewichtung. Weil die Formen alle konvex sind, ist garantiert, dass die gebildeten Punkten alle innerhalb der Meshes liegen.

Um schliesslich einen schönen Effekt zu erzielen, wurden noch ein paar zusätzliche Details eingebaut:

\begin{itemize}
	\item Die Partikel sind animiert
	\item Die Partikel erhalten die Farbe des aktuellen Modus'
	\item Die Partikel werden beim Verlieren der Handerkennung zerstreut
\end{itemize}

Alles in allem wurde damit eine ansprechende und intuitive Darstellung der Hände erreicht.

\subsection{Point Chart}

Wenn man von Datenvisualisierung redet, ist es ganz klar, dass auch Diagramme dargestellt werden wollen. Umso besser, wenn dies stereoskopisch geschehen kann: Dadurch lässt sich die dritte Dimension erheblich besser wahrnehmen.

Im Rahmen dieses Projekts wurde ein Point-Chart entwickelt, der dazu dient, die Musik des Anwenders in ein dreidimensionales System einzuordnen und auf einen Blick darzustellen. Die Dimensionen sind 1:1 die Features, die während der Indexierung von The Echo Nest heruntergeladen wurden, also Merkmale wie \textit{Danceability}, \textit{Energy} und \textit{Tempo}.

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{bilder/pointchart}
	\caption{Visualisierung der Musiksammlung innerhalb eines Point Charts}
	\label{fig:pointchart}
\end{figure}

Wie auf dem UML ersichtlich, wurde die Implementation des Point Charts in drei verschiedenen Klassen durchgeführt.

\code{PointChart} ist eine relativ generisch gehaltene Implementation eines Point Charts, welche sich nur um die Darstellung kümmert.

\code{SongMetaChart} ist eine Komponente, die auf der ersten basiert und diese steuert, und die Metadaten-Logik ins Spiel bringt.

\code{SelectionVisualizer} ist eine weitere Komponente, die ihrerseits auf \code{SongMetaChart} aufbaut und die momentane Selektion (nur im Listen-Modus) visualisiert.

Die zentrale Klasse der Funktionalität, \code{PointChart}, besteht visuell ebenfalls aus drei Teilen. Es werden drei Elemente bei Änderungen prozedural erstellt:

\begin{itemize}
	\item Ein Koordinatensystem, das aus drei Balken besteht, welche mit \textit{Triangles} generiert werden
	\item Ein dreidimensionales Gitternetz, welches mit Linien erstellt wird
	\item Einer Punktewolke aus Partikeln
\end{itemize}

Die zwei Meshes werden grundsätzlich nur einmal generiert, können aber mithilfe der Methode \code{RebuildMesh()} jederzeit neu erzeugt werden. Die Linien des Gitters bestehen aus einem durchsichtigen Material-Shader, welcher ihnen eine passend subtile Sichtbarkeit gibt.

Das Partikelsystem, welches für die Punkte des Diagramms zuständig ist, sollte immer bei Änderungen der Punktwerte mithilfe der Methode \code{UpdatePoints()} erneuert werden.

Ursprünglich war geplant, diese Punkte durch ein \textit{Point}-Rendering zu implementieren, doch diese Art des Renderings ist im Falle von Unity relativ limitiert. Will man lediglich Punkte als einzelne Pixel darstellen, stellt das kein Problem dar. Problematisch wird es, sobald die Grösse der Punkte geändert werden soll, denn dann ist man auf einen OpenGL-Build angewiesen \cite{psize}.

Die Implementation per Partikelsystem funktioniert gut. Auf was geachtet werden muss, ist die Lebenszeit der einzelnen Partikel. Da sie in diesem Fall nicht kurzlebig sind sondern persistent, wird ihre Lebenszeit bei der Emission der Partikel auf einen Wert gesetzt, der mit sehr hoher Wahrscheinlichkeit nie erreicht werden wird.

\lstinputlisting[caption={Erstellung der Punkte in einem PointChart}, 
label={lst:pointchart}]{code/PointChart_UpdatePoints.cs}


\subsection{Farben und Symbolik}
%TODO: Themes

\subsection{Beat Detector und Wellen}
\label{subsec:waves}

Damit die Szene, in der sich der Anwender wiederfindet, nicht zu steril und still ist, wurde ein sehr rudimentärer Beat Detector implementiert, der bei jedem Beat eine Welle mit entsprechender Stärke aussendet.

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{bilder/beat-waves}
	\caption{Beats werden mithilfe von Wellen dargestellt (rot umrahmt)}
	\label{fig:beat-waves}
\end{figure}

Für die Programmierung der Beat-Detection wurde ein Artikel von GameDev.com zurate gezogen \cite{BeatDetection}. Ganz grob gesagt, wird einfach nach Peaks im Energieverlauf der Musik gesucht. Die benötigten Werte werden alle durch die selbsterstellte Helper-Klasse \code{VisualizationHelper} bereitgestellt. Als kleine Veränderung zum ursprünglichen Algorithmus wurde die Suche nach Beats auf das Frequenzspektrum zwischen 50Hz und 200Hz beschränkt. %TODO: Glossary?

Anstatt einer dynamischen Beat-Detection hätte auch ein Pre-Processing durchgeführt bzw. die Werte von The Echo Nest verwendet werden können. Allerdings ist eine so genaue Beat Detection in diesem Fall nicht notwendig, und für ein exaktes Timing wäre eine stetige Synchronisation zwischen Daten und Musik notwendig.

Die Ringe, die ausgesendet werden, werden alle prozedural generiert und passen sich der \textit{Theme}-Farbe an. Das System basiert auf drei Klassen:

\begin{itemize}
	\item Der \code{BeatDetector} versendet bei Beats eine \code{OnBeat(strength)} Nachricht an Komponent im gleichen \textit{GameObject}.
	\item Der \code{WaveEmitter} fängt die Nachricht ab und erstellt ein neues \textit{GameObject} mit einer \code{RingMesh}-Komponente.
	\item \code{RingMesh} sorgt dann schliesslich dafür, dass der Kreisring bzw. das Kreissegment korrekt dargestellt wird.
\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{diagramme/ringmesh}
	\caption{Die Ring-Meshes werden mithilfe von \textit{Quads} generiert}
	\label{fig:ringmesh}
\end{figure}

Die Erstellung des Kreisrings ist sehr simpel gehalten. Die Komponente erlaubt das Konfigurieren von Innenradius, Aussenradius, Startwinkel, Winkelbereich und Anzahl Segmenten. Diese Eigenschaften werden überwacht, und bei jeder Änderung wird beim nächsten Update der Engine eine Neugenerierung des Meshes ausgelöst.

Bei der Generierung wird ein Array der Länge $n_\textit{Segmente}*2$ erstellt, welches die Vertices enthält. %TODO: +2!
Die Punkte des Aussen- und Innenkreis werden dann gleichzeitig in dieses Array gefüllt, wobei der Aussenkreis einen Offset von $n_\textit{Segmente}$ erhält (siehe Abbildung \ref{fig:ringmesh}). Schliesslich werden dann die Quads mit jeweils vier Indexen gebildet.
%TODO: Prüfe Richtung


\subsection{Visualisierung der Musik}

\subsection{Avatar}

Auch der Anwender selbst muss irgendwie dargestellt werden. Ein Avatar gilt generell als ein Hilfsmittel, um die Immersivität zu steigern, kann jedoch auch verschlechternd wirken \cite{oculus:bestpractices}.

Im Falle von IMVR wurde jedoch die Entscheidung getroffen, einen simplen Avatar einzubauen. Aufgrund der Art der Applikation, fiel hierbei die Wahl auf eine abstrakte Figur, die frei verfügbar und vollständig geriggt ist. Dadurch bleibt jederzeit die Möglichkeit offen, per \gls{ik} die Figur zu steuern.

Da die Hände und der Kopf nicht benötigt werden, wurden diese kurzerhand mithilfe von 3DS Max 2015 demontiert. Die vorherig erwähnte Kontrolle über IK wurde nicht implementiert, könnte aber in einem nächsten Schritt eingebaut werden.

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{bilder/avatar}
	\caption{Der verwendete Avatar mit Kopf und Armen}
	\source{http://www.turbosquid.com/FullPreview/Index.cfm/ID/833108}
	\label{fig:avatar}
\end{figure}



\section{Herausforderungen}

\subsection{Ressourcen-Management}
\label{subsec:resources}

\subsection{Abspielen externer Musik}
\label{subsec:musicformats}

\subsection{Canvas-Elemente}

\subsection{Auswerten der Musik}
%TODO: CSCore, NAudio, FFT

% Alpha-Sorting, etc. Tile-Blanket