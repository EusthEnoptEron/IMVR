\chapter{Implementation von IMVR}
\label{chap:implementation_imvr}

Der Indexer ist implementiert und dokumentiert, also fehlt nur noch die eigentliche Applikation. Vieles hat sich im Laufe des Projekts verändert und entwickelt, und viele Probleme traten zum Vorschein, die an dieser Stelle genauer erläutert werden sollen.

\section{Unity 5}

Die Implementation von IMVR lässt sich leider nicht ohne einen gewissen Hintergrund in Unity 5 erklären. 

\subsection{Grundkonzepte}

Unity ist eine Entwicklungsumgebung und eine Spiel-Engine, die momentan aufgrund ihrer Bedienungsfreundlichkeit  und einer  frei erhältlichen  Version sehr  beliebt in  der Szene  der Indie-Developer ist. Gleichzeitig dient sie auch als gutes Prototyping-Tool, um schnell Ideen umzusetzen. 

Um Unity in groben Zügen zu erklären, sollen zwei Ansichtspunkte beschrieben werden: der Szenenaufbau und die Ressourcenverwaltung. 

Ein  Projekt  in  Unity  ist  in  sogenannte  \textit{Scenes}  (Szenen)  gegliedert,  welche  aus  Objekten (\textit{GameObject})  bestehen  –  das  Grundprinzip  der  Scene-Graphs  wird  also  angewandt. Speziell  ist,  dass  die  Interaktion  und  Spiellogik  grundsätzlich  nur  innerhalb  von \textit{Components}  geschieht.  Jedes  Script  und  jede  \textit{Eigenschaft}  wird  als  Component  einem GameObject  zugeordnet.  So  hat  zum  Beispiel  ein  Licht  ein  \textit{Light}-Component  oder  die Kamera ein \textit{Camera}-Component. Selbst die Position jedes GameObjects ist nur ein Wert im  \textit{Transform}-Component.

Ein  anderer  Aspekt  von  Unity  ist  die  Ressourcenverwaltung.  Im  Grunde  genommen  ist  es 
dem  Programmierer  überlassen,  wie  er  seine  Ressourcen  verwaltet.  Das  einzige,  was 
beachtet  werden  muss,  ist,  dass  alle  Ressourcen  im  \textit{Assets}-Folder  abgelegt  werden 
müssen.  Üblicherweise  wird  dann  ein  Ordner  für  jede  Art  von  Asset  erstellt,  z.B.  für 
Materialien, Texturen, Meshes, Scripts, etc.

Ein  weiterer  wichtiger  Begriff  sind  die  \textit{Prefabs}.  Damit  sind  Vorlagen  gemeint,  die  man 
erstellt, indem man fertige Objekte aus der aktuellen Szene in das Asset-Folder zieht, und 
danach wiederverwerten kann. 

\subsection{Unity im Kontext von IMVR}

Was sofort auffällt bei der Betrachtung der Ressourcenverwaltung, ist, dass diese starke Kopplung von Assets zu Projekten sich mit der grundlegenden Aufgabe dieses Projektes beisst. Unity sieht vor, dass der Programmierer während der Entwicklung alle seine Assets in den dafür vorgesehenen Ordner platziert, das Projekt am Schluss kompiliert, und dann höchstens im Nachhinein neue Assets als \textit{Asset Bundles} an seine Anwender verteilt. In diesem Projekt ist es jedoch zwingend nötig, dynamisch Bilder und Musik anhand der Dateien auf dem Anwender-PC zu laden. Auf die Folgen und Lösungen zu diesem Problem wird in den Kapiteln \ref{subsec:resources} und  \ref{subsec:musicformats} näher eingegangen.

\subsection{UI}

Ein Feature, von dem die Applikation starken Gebrauch macht, ist das UI-System von Unity. Vor Version 4.6 des Editors wurde das GUI grösstenteils im Programmcode selbst erstellt mithilfe des alten System \textit{nGUI}.

Das neue System ist analog zu nGUI unter dem Namen \textit{uGUI} bekannt. Neuerdings werden GUI-Elemente direkt im Editor platziert und können von einer umfangreichen Layout-Engine profitieren. Ausserdem verfügen UI-Komponenten über ihre eigene Behaviour-Klasse \code{UIBehaviour}.

Der entscheidende Vorteil davon im Kontext dieses Projektes ist die Möglichkeit, diese UI-Elemente dreidimensional darzustellen. Neben einem Overlay- und einem perspektivischen Kamera-Modus verfügt ein Canvas, das Root-Element eines UIs, über einen World-Modus, wo das Canvas direkt in der Welt positioniert wird.

Eine Implikation, die dieses System in sich birgt, ist, dass es nicht mehr so einfach wie zu Zeiten von nGUI ist, ein GUI im Code zu erstellen. Würde man alle Elemente dynamisch generieren, würde dies zu unleserlichem und schlecht handhabbaren Code führen.

Um dieses Problem zu umgehen, bieten sich Prefabs an. Der Prozess der Erstellung eines einzelnen UIs sieht folgendermassen aus:

\begin{enumerate}
	\item Designen der UI im Editor
	\item Speichern als Prefab
	\item Im Code: Laden und per \code{SetTransform()} an ein anderes Element anhängen
	\item Iterieren bis fertiggestellt
\end{enumerate}


\section{Aufbau}

%TODO: Unity-Struktur

\section{Interaktionskonzept}

Unüberraschenderweise findet die Interaktion des Users mit IMVR fast ausschliesslich mit seinen Händen statt. In einer frühen Phase des Projektes war noch geplant, eventuell die Spracheingabe modal zu den Händen zu gebrauchen, jedoch reichte dafür die Zeit nicht mehr. Ein Artefakt dieses Vorhabens ist das \textit{SpeechServer}-Projekt, welches sich immer noch unter den \textit{AuxiliaryTools} befindet.

\subsection{Fussplatten}

Beim Erstellen einer visuellen Applikation stellt sich die Frage, wie man am besten die Struktur verdeutlichen kann. Eine Technik, die dafür gewählt wurde, ist der Einsatz von "Fussplatten".

Hierbei befinden sich unter den Füssen des Anwenders ringförmige Platten, welche die zwei Modi der Applikation repräsentieren. Eine dritte, zentriert abgehobene Platte dient zur Beendigung der Applikation.

Bei diesen Platten handelt es sich um das einzige Interaktionsmittel, welches bewusst keine Eingabe durch die Hände erfordert. In diesem Fall wird die Oculus Rift selbst als Eingabegerät verwendet, und zwar durch den Blickwinkel.

Die Idee ist, dass der Benutzer feststellen will, ``wo er steht'', herunterschaut, und durch gehaltenen Blickkontakt mit den Fussplatten diese aktivieren kann. Entsprechend den \textit{Best Practices} wird dabei ein Indikator gefüllt, der anzeigt, wie lange der Blick noch gehalten werden muss.

Diese Art von Eingabe lässt sich oft bei bereits erschienen Demos für die Oculus Rift beobachten. Das Prinzip ist sehr leicht zu implementieren und daher auch verlockend, jedoch muss mit Vorsicht vorgegangen werden: Für den Benutzer ist es auf Dauer unangenehm, wenn von ihm ständige Kopfbewegungen gefordert werden. In IMVR wurde jedoch bewusst Gebrauch von dieser Methode gemacht, weil der Anwender nur selten nach unten schauen wird, und es relativ intuitiv ist.

%TODO: note that ring mesh can be seen in \ref{subsec:waves}


\subsection{Ringmenü}

Bei der Entwicklung von VR-Applikationen stösst man zwingenderweise auf Situationen, in denen herkömmliche Konzepte nicht mehr verwendet werden können. Die Platzierung und der Aufbau des Menüs ist so ein Punkt.

Es ist nicht leicht ein Menü korrekt zu platzieren. Eine statische Platzierung als Overlay hält das Interface zwar im sichtbaren Bereich, kann sich jedoch als "lästig" herausstellen. Lässt man es verzögert mitschweben, gerät das Menü sofort ausser Kontrolle, und stellt man es irgendwo in die Szene und belässt es dabei, verliert man es sofort aus dem Blick.

Im Falle von IMVR bieten sich jedoch die Hände als gut verwendbarer Ankerpunkt für das Menü an. Es gibt ein freies Projekt auf GitHub \cite{hoverVR}, welches die Finger der Hand für die Platzierung der Buttons in einer ringartigen Struktur verwendet. Leider befand sich das Projekt in einem zu instabilen Stadium für diese Arbeit, aber es lieferte die Idee für eine eigene, ähnliche Implementierung.

Für IMVR wurde ebenfalls ein Ringmenü entwickelt, doch dieses verfügt über keine Schaltflächen im herkömmlichen Sinn. Die Finger werden auch mit Funktionen versehen, aber zum Betätigen benutzt der Anwender nicht seine andere Hand, sondern hebt einen Finger. Wenn ein Finger lange genug gehoben wird, wird die zugewiesene Aktion ausgeführt.

\subsection{Music Arm}

Da es in dieser Arbeit voll und ganz um das Eingreifen ins Geschehen mit den eigenen Händen geht, wäre es bedauerlich, wenn es nicht möglich wäre, die Musik mit den Händen zu steuern. Ein weiteres Konzept, welches genau dieses Problem löst, ist der sogenannte "Music Arm".

Beim Music Arm handelt es sich um das Interface, welches an Stelle des menschlichen Arms angezeigt wird. Dieser digitale Arm bietet die Möglichkeit, die Playlist zu verwalten, sowie die momentan abgespielte Musik anzuschauen bzw. vor- und zurückzuspulen.

Hierbei wird eine Technik verwendet, welche einen Unterschied zwischen der Vorder- und der Rückseite macht. Schaut der Benutzer auf die ``Uhr'', also hält seinen Handrücken vor sich, dann erscheint ein Interface, in welchem er alle wichtigen Informationen über das momentane Lied erhält (Name, Artist, Cover, Länge und Fortschritt).


\begin{figure}[h]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{bilder/musicarm_front}
		\caption{Vorderseitenmenü}
		\label{fig:musicarm_front}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.5\linewidth]{bilder/musicarm_back}
		\caption{Rückseitenmenü}
		\label{fig:musicarm_back}
	\end{subfigure}
	\caption{Darstellung des Music Arms}
	\label{fig:musicarm_frontback}
\end{figure}


Dreht der Anwendet jedoch seinen Arm und schaut auf die Rückseite, wird er eine Liste erhalten, die er scrollen und  selektieren kann. Bei der Selektion wird das Element auf das Ringmenü "gesendet", wo der Anwender dann die Möglichkeit hat, das Lied abzuspielen, oder die Selektion aufzuheben. Würde man sofort bei der Selektion ein Lied abspielen, ergäbe sich das Problem, dass durch die mangelhafte Genauigkeit der Leap Motion und der Bedienung allgemein viele Fehlselektionen passieren würden und somit diverse Lieder zufällig abgespielt würden. Abbildung \ref{fig:musicarm_frontback} zeigt, wie dieses Interface aussieht.

Der Entscheid, welche Seite angezeigt wird, geschieht über ein simples Skalarprodukt. Der Forward-Vektor der Kamera wird verglichen mit der Normale des Arms, welche in die gleiche Richtung zeigt, wie der Handrücken. Wenn der Wert tiefer als -0.5 ist, die Vektoren also entgegengerichtet sind, wird die Unterseite erkannt, und wenn der Wert höher als 0.5 ist, die Oberseite. Der Entscheid könnte auch bei 0 gefällt werden, aber dann würden auch Grenzfälle angezeigt werden, in denen besser gar kein Menü angezeigt wird.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\linewidth]{diagramme/musicarm}
\caption{Bestimmung der momentanen Armseite}
\label{fig:musicarm}
\end{figure}

\subsection{Slider Balken}

Im Hörteil der Applikation, dem blauen Teil, sollte es dem Anwender ermöglicht werden, die Features der gewünscheten Musik anzugeben, und diese dann anzuhören. Wie bereits erwähnt, bilden diese Werte einen Bereich zwischen 0 und 1 ab (mit Ausnahme des Tempos). Will der Anwender eine energievolle Musik hören, muss es ihm deshalb möglich sein, neben dem Maximalwert auch einen Minimalwert anzugeben. Es liegt also ein typischer Use-Case für einen Range-Slider vor.

Da sich kein solcher im Werkzeugkasten von Unity befindet, musste ein eigener erstellt werden. Um einen anderen Approach zu testen, als mit der starken Verwendung von Unitys neuem UI-System in den anderen Teilen der Applikation gewählt wurde, wurde in diesem Fall bewusst ein Slider mithilfe eines Zylinder-Meshes erstellt.

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{bilder/cylinder-slider}
	\caption{In-Game Screenshot zur Bedienung der Slider-Balken}
	\label{fig:cylinder-slider}
\end{figure}


Beim implementierten Slider hat der Anwender die Möglichkeit, direkt mit seinen zwei Händen einen Auswahlbereich einzustellen, indem er diese parallel in den Zylinder hält und entsprechend bewegt. Anders als die relativ abstrakte Implementation der anderen Interaktionselemente, basiert dieser Slider auf das Kollisionsmodell von Unity und die physikalische Hände, die mit dem Leap Motion Plugin mitgeliefert werden.

Das System funktioniert so, dass zu Beginn eine Achse $\hat{v}_\textit{dir}$ festgelegt wird, in die sich der Zylinder füllt (z.B. von links nach rechts). Bei einer Kollision der Hand mit dem Zylinder, wird nun zuerst geprüft, ob die Hand gültig ist. Sofern das der Fall ist, wird die Position der Handfläche $\vec{P}_\textit{hand}$ auf die Achse $\hat{v}_\textit{dir}$ projiziert und auf die Länge des Zylinders skaliert.

Der Wert, der dadurch entsteht, wird dann je nach geprüfter Handseite als neues Minimum bzw. Maximum verwendet. Allerdings geschieht davor noch eine lineare Interpolation vom vorigen Wert für einen weichen Übergang und ein Snapping zum Vereinfachen der Auswahl des Maximums bzw. Minimums.

Damit andere Teile der Applikation auf diese Wertänderungen reagieren können, wird beim Ändern der Extrema jeweils die Events \code{onMinValueChanged} bzw. \code{onMaxValueChanged} sowie \code{onValueChanged} ausgelöst. Durch Abfangen dieser Events wird z.B. der Zähler im \textit{Selector} bei Änderungen der Selektion aktualisiert.

%\begin{algorithm}
%	\caption{My algorithm}
%	\begin{algorithmic}[1]
%		\Procedure{MyProcedure}{}
%		\State $H \gets Hand$
%		\State $\vec{P}_{hand} \gets \text{Position der Hand}$
%		\State $\vec{P}_{min} \gets \text{Position wenn leer}$
%		\State $\vec{P}_{max} \gets \text{Position wenn voll}$
%		\State $\hat{v}_{dir} \gets \frac{\vec{P}_{max}-\vec{P}_{min}}{|\vec{P}_{max}-\vec{P}_{min}|}$
%		\If {$\text{isValid(}\textit{H}\text{)}$}
%		\State $j \gets j-1$.
%		\State $i \gets i-1$.
%		\State \textbf{goto} \emph{loop}.
%		\State \textbf{close};
%		\EndIf
%		\State $i \gets i+\max(\textit{delta}_1(\textit{string}(i)),\textit{delta}_2(j))$.
%		\State \textbf{goto} \emph{top}.
%		\EndProcedure
%	\end{algorithmic}
%\end{algorithm}


\section{Visual Design}

Viele Elemente in IMVR haben mit dem Visuellen zu tun. Sie sollen die Musik unterstützend begleiten, Verhältnisse darstellen und natürlich gefallen.

\subsection{Darstellung der Hände}
\label{subsec:handfx}

Das erste Thema, das beim Visuellen einfällt, sind die Hände. Bei der Gestaltung der Hände waren grundsätzlich folgende Voraussetzungen zu erfüllen:

\begin{enumerate}
	\item handförmigs
	\item abstrakt
	\item momentaner Modus ist erkennbar
\end{enumerate}

Der erste Punkt versteht sich von selbst. Der zweite Punkt, die Abstraktheit, kommt daher, weil es am besten zur Applikation passt. Eine echt-aussehende, materielle Hand würde fehl am Platz wirken, wenn der Rest der Szene fast ausschliesslich aus Geometrie besteht und keinen Zusammenhang mit der Realität hat.

Neben dem erwähnten Argument, hat eine abstrakte Hand auch den Vorteil, dass die Darstellung der momentan Musik angepasst werden kann, z.B. durch Farbe oder Bewegung. % TODO: Ist sowas drin? 

Beim dritten Punkt ist mit \textit{Modus} der aktuelle Applikationsmodus gemeint, also der Browse- bzw. Listen-Modus. Diese werden unter Anderem durch die Farbe unterschieden, und diese Farbe zeigt sich auch bei der Visualisierung der Hände.

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{bilder/hand1}
	\caption{Die ursprüngliche Visualisierung der Hand}
	\label{fig:hand1}
\end{figure}

Für die erste Art der Visualisierung (Abbildung \ref{fig:hand1}) wurde eine leicht abgeänderte Form einer Standardhand im Leap Motion Package gewählt. Dabei handelte es sich um eine auf Voxel basierende Darstellung. Für jeden Knochen in der Hand wurde ein Voxel-Sheet erstellt, welches dann den Bewegungen der Hand gefolgt ist. In der Implementation in IMVR wurden lediglich die Voxels mit Kugeln ersetzt, um ein bisschen Individualität zu schaffen.

Diese Darstellungsform wurde jedoch im Laufe des Projekts zugunsten einer partikelbasierten Implementation verworfen. In der ersten partikelbasierten Form, wurde versucht eine geriggte Hand mit Partikeln zu umrahmen statt das Mesh selbst zu rendern. Das war jedoch mit ein paar grossen Nachteilen verbunden:

\begin{enumerate}[label=(\alph*)]
	\item Die Bone-Weights werden auf der GPU appliziert, deshalb muss das Mesh in jedem Frame auf der CPU nachgebildet werden.
	\item Durch die hohe Zahl der Vertices werden über 10\,000 Partikel pro Hand erstellt, was sich sehr schlecht auf die Performance auswirkt.
	\item Da das Finden von Punkten innerhalb eines komplizierten Meshes nicht trivial ist, müssen alle Punkte auf der Hülle platziert werden.  
\end{enumerate}

Aufgrund dieser Nachteile wurde die Implementation schliesslich noch einen Schritt weiter abgeändert. In der finalen Visualisierung (Abbildung \ref{fig:hand2}) wird eine abstrakte Hand, welche aus mehreren Meshes besteht, mit Partikeln \textit{gefüllt}. Damit erhält der Anwender ein intuitives Feedback, wenn die Hand erkannt wird, anstatt dass diese aus dem Nichts erscheint. Damit er trotzdem nicht auf den Effekt warten muss, erhält die Hand einen halb-durchsichtigen Rahmen.

\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{bilder/hand2}
	\caption{Die finale Visualisierung der Hand mit Partikeln}
	\label{fig:hand2}
\end{figure}

Ein Grossteil des Codes konnte von der vorhergehenden Implementation mit dem geriggten Modell übernommen werden. Der Vorteil hier ist, dass die Hand auf \textit{Transforms} aufbaut: Die Submeshes, ein Mesh pro Knochen, werden durch ihre \textit{Transforms} korrekt platziert, damit eine Hand gebildet wird. Dadurch entsteht zwar nicht ein fliessender Übergang wie bei einer geriggten Hand, aber man kann mit primitiven Elementen arbeiten.

In diesem Fall wurde ein Handmodell gewählt, welches grösstenteils aus einfachen, abgerundeten Quadern besteht. Bei der Erstellung der Hand, werden alle Meshes abgelaufen und für jedes Mesh eine gewisse Anzahl Partikel erstellt. Damit diese \textit{innerhalb} der Hand erscheinen, und nicht auf der Hülle wie bei der vorherigen Implementation, wird jede Partikelposition aus zwei zufälligen, verschiedenen Vertices gebildet mit einer ebenfalls zufälligen Gewichtung. Weil die Formen alle konvex sind, ist garantiert, dass die gebildeten Punkten alle innerhalb der Meshes liegen.

Um schliesslich einen schönen Effekt zu erzielen, wurden noch ein paar zusätzliche Details eingebaut:

\begin{itemize}
	\item Die Partikel sind animiert
	\item Die Partikel erhalten die Farbe des aktuellen Modus'
	\item Die Partikel werden beim Verlieren der Handerkennung zerstreut
\end{itemize}

Alles in allem wurde damit eine ansprechende und intuitive Darstellung der Hände erreicht.

\subsection{Point Chart}
\label{subsec:pointchart}

Wenn man von Datenvisualisierung redet, ist es ganz klar, dass auch Diagramme dargestellt werden wollen. Umso besser, wenn dies stereoskopisch geschehen kann: Dadurch lässt sich die dritte Dimension erheblich besser wahrnehmen.

Im Rahmen dieses Projekts wurde ein Point-Chart entwickelt, der dazu dient, die Musik des Anwenders in ein dreidimensionales System einzuordnen und auf einen Blick darzustellen. Die Dimensionen sind 1:1 die Features, die während der Indexierung von The Echo Nest heruntergeladen wurden, also Merkmale wie \textit{Danceability}, \textit{Energy} und \textit{Tempo}.

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{bilder/pointchart}
	\caption{Visualisierung der Musiksammlung innerhalb eines Point Charts}
	\label{fig:pointchart}
\end{figure}

Wie auf dem UML ersichtlich, wurde die Implementation des Point Charts in drei verschiedenen Klassen durchgeführt.

\code{PointChart} ist eine relativ generisch gehaltene Implementation eines Point Charts, welche sich nur um die Darstellung kümmert.

\code{SongMetaChart} ist eine Komponente, die auf der ersten basiert und diese steuert, und die Metadaten-Logik ins Spiel bringt.

\code{SelectionVisualizer} ist eine weitere Komponente, die ihrerseits auf \code{SongMetaChart} aufbaut und die momentane Selektion (nur im Listen-Modus) visualisiert.

Die zentrale Klasse der Funktionalität, \code{PointChart}, besteht visuell ebenfalls aus drei Teilen. Es werden drei Elemente bei Änderungen prozedural erstellt:

\begin{itemize}
	\item Ein Koordinatensystem, das aus drei Balken besteht, welche mit \textit{Triangles} generiert werden
	\item Ein dreidimensionales Gitternetz, welches mit Linien erstellt wird
	\item Einer Punktewolke aus Partikeln
\end{itemize}

Die zwei Meshes werden grundsätzlich nur einmal generiert, können aber mithilfe der Methode \code{RebuildMesh()} jederzeit neu erzeugt werden. Die Linien des Gitters bestehen aus einem durchsichtigen Material-Shader, welcher ihnen eine passend subtile Sichtbarkeit gibt.

Das Partikelsystem, welches für die Punkte des Diagramms zuständig ist, sollte immer bei Änderungen der Punktwerte mithilfe der Methode \code{UpdatePoints()} erneuert werden.

Ursprünglich war geplant, diese Punkte durch ein \textit{Point}-Rendering zu implementieren, doch diese Art des Renderings ist im Falle von Unity relativ limitiert. Will man lediglich Punkte als einzelne Pixel darstellen, stellt das kein Problem dar. Problematisch wird es, sobald die Grösse der Punkte geändert werden soll, denn dann ist man auf einen OpenGL-Build angewiesen \cite{psize}.

Die Implementation per Partikelsystem funktioniert gut. Auf was geachtet werden muss, ist die Lebenszeit der einzelnen Partikel. Da sie in diesem Fall nicht kurzlebig sind sondern persistent, wird ihre Lebenszeit bei der Emission der Partikel auf einen Wert gesetzt, der mit sehr hoher Wahrscheinlichkeit nie erreicht werden wird.

\lstinputlisting[caption={Erstellung der Punkte in einem PointChart}, 
label={lst:pointchart}]{code/PointChart_UpdatePoints.cs}


\subsection{Farben und Symbolik}

Um die einzelnen Modi mit Farben abzugrenzen, wurden \textit{Themes} eingeführt, die jeweils eine Kollektion von Farben beinhalten. Beim Aktivieren eines Modus wird dieses Theme ebenfalls aktiviert - zugreifbar per \code{Theme.Current} - und auf die UI-Elemente angewendet.

Als farbliche Vorlage wurde das UI von Android \cite{android_ui} gewählt. Farben und UI-Ideen wurden teilweise aus den Spezifikation des Material-Designs entnommen.

Damit alle Elemente auf einen Themenwechsel reagieren können, verfügt die Klasse \code{Theme} über ein \code{Change}-Event.

\subsection{Beat Detector und Wellen}
\label{subsec:waves}

Damit die Szene, in der sich der Anwender wiederfindet, nicht zu steril und still ist, wurde ein sehr rudimentärer Beat Detector implementiert, der bei jedem Beat eine Welle mit entsprechender Stärke aussendet.

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{bilder/beat-waves}
	\caption{Beats werden mithilfe von Wellen dargestellt (rot umrahmt)}
	\label{fig:beat-waves}
\end{figure}

Für die Programmierung der Beat-Detection wurde ein Artikel von GameDev.com zurate gezogen \cite{BeatDetection}. Ganz grob gesagt, wird einfach nach Peaks im Energieverlauf der Musik gesucht. Die benötigten Werte werden alle durch die selbsterstellte Helper-Klasse \code{VisualizationHelper} bereitgestellt. Als kleine Veränderung zum ursprünglichen Algorithmus wurde die Suche nach Beats auf das Frequenzspektrum zwischen 50Hz und 200Hz beschränkt. %TODO: Glossary?

Anstatt einer dynamischen Beat-Detection hätte auch ein Pre-Processing durchgeführt bzw. die Werte von The Echo Nest verwendet werden können. Allerdings ist eine so genaue Beat Detection in diesem Fall nicht notwendig, und für ein exaktes Timing wäre eine stetige Synchronisation zwischen Daten und Musik notwendig.

Die Ringe, die ausgesendet werden, werden alle prozedural generiert und passen sich der \textit{Theme}-Farbe an. Das System basiert auf drei Klassen:

\begin{itemize}
	\item Der \code{BeatDetector} versendet bei Beats eine \code{OnBeat(strength)} Nachricht an Komponent im gleichen \textit{GameObject}.
	\item Der \code{WaveEmitter} fängt die Nachricht ab und erstellt ein neues \textit{GameObject} mit einer \code{RingMesh}-Komponente.
	\item \code{RingMesh} sorgt dann schliesslich dafür, dass der Kreisring bzw. das Kreissegment korrekt dargestellt wird.
\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{diagramme/ringmesh}
	\caption{Die Ring-Meshes werden mithilfe von \textit{Quads} generiert}
	\label{fig:ringmesh}
\end{figure}

Die Erstellung des Kreisrings ist sehr simpel gehalten. Die Komponente erlaubt das Konfigurieren von Innenradius, Aussenradius, Startwinkel, Winkelbereich und Anzahl Segmenten. Diese Eigenschaften werden überwacht, und bei jeder Änderung wird beim nächsten Update der Engine eine Neugenerierung des Meshes ausgelöst.

Bei der Generierung wird ein Array der Länge $n_\textit{Segmente}*2$ erstellt, welches die Vertices enthält. %TODO: +2!
Die Punkte des Aussen- und Innenkreis werden dann gleichzeitig in dieses Array gefüllt, wobei der Aussenkreis einen Offset von $n_\textit{Segmente}$ erhält (siehe Abbildung \ref{fig:ringmesh}). Schliesslich werden dann die Quads mit jeweils vier Indexen gebildet.
%TODO: Prüfe Richtung


\subsection{Visualisierung der Musik}
\label{subsec:visualization}

Damit das Eintauchen in die Musik nicht zu langweilig und unspektakulär wird, wurden ein paar Klassen für die Musikvisualisierung entwickelt.

In einem ersten Schritt wurde mithilfe von NAudio \cite{naudio} versucht, selbst Daten über den Frequenzbereich der momentanen Musik per FFT zu sammeln. Dies funktionierte grundsätzlich, brachte aber den Nachteil mit sich, dass ausschliesslich NAudio für die Wiedergabe genutzt werden konnte.

Es stellte sich dann heraus, dass Unity mit \code{GetOutputData} und \code{GetSpectrumData} selbst über eine Analysefunktion für Musik verfügt\footnote{Dokumentation: http://docs.unity3d.com/ScriptReference/AudioSource.GetSpectrumData.html}, und das sogar mit verschiedenen Analysefenstern.

Um die gewonnenen Daten zentral zu verwalten, wurde nun eine Klasse namens \code{VisualizationHelper} erstellt. Diese hilft ausserdem dabei, die momentane Lautstärke der Stereo-Kanäle zu bestimmen, indem sie ein RMS-Wert über alle Samples erstellt \cite{outputData}.

%TODO: Glossary
Es gibt drei verschiedene Arten, wie die Musik momentan visualisiert wird:

\begin{enumerate}
	\item die Beats
	\item die Lautstärke der Stereokanäle (Waveform)
	\item die Lautstärke von Frequenzbereichen
\end{enumerate}

Wie die Detektion und Visualisierung von Beats durchgeführt wird, wurde bereits in Kapitel \ref{subsec:waves} beschrieben.

Für die Lautstärke der Stereokanäle wird mit dem Linienrenderer von Unity dynamisch eine Welle generiert, die einem Kreis entlangläuft. Die Klasse \code{RingVisualizer} steuert hierbei den Renderer. Die Implementation ist äussert simpel:

\lstinputlisting[caption={Aktualisierung der Punkte im \code{RingVisualizer}}, 
label={lst:ringvisualizer}]{code/RingVisualizer_Update.cs}

Jedem Punkt im Line-Renderer wird ein sich verschiebender Volume-Wert zugewiesen, der vom \code{VisualizationHelper} bereitgestellt wird. Hierbei gibt dieser Volume-Wert die Position auf der Y-Achse vor (die Höhe), während die Position auf der XZ-Ebene per Sinus und Kosinus bestimmt wird.

Etwas interessanter ist jedoch die Berechnung der Lautstärke in einem bestimmten Frequenzbereich.

\lstinputlisting[caption={Berechnung des RMS-Wertes über einen Frequenzbereich}, 
label={lst:ringvisualizer}]{code/SpectrumBar_Update.cs}

Diese Berechnung wird in der Klasse \code{SpectrumBar} durchgeführt. Wie auf dem UML (Abbildung \ref{uml:IMVR.FX}) zu sehen ist, verfügt jede Instanz über eine Start- und eine Endfrequenz, der den Frequenzbereich festlegt. Dieser Bereich sollte mit höheren Frequenzen unbedingt logarithmisch anwachsen, da sonst die Kurve ungleichmässig verteilt ist.

Zuerst wird im Code dieser Frequenzbereich in Indexe für das \code{Spectrum}-Array umgewandelt. Die Grösse dieses Arrays, und somit die Abstände der Frequenzen, ist abhängig von der Anzahl Samples, die für den FFT eingesetzt werden.

Die Werte, die zwischen den zwei Indexen liegen, werden dann quadratisch akkumuliert. Der Durchschnitt des Resultats wird dann schliesslich für die Höhe eines Quaders verwendet.

\subsection{Avatar}

Auch der Anwender selbst muss irgendwie dargestellt werden. Ein Avatar gilt generell als ein Hilfsmittel, um die Immersivität zu steigern, kann jedoch auch verschlechternd wirken \cite{oculus:bestpractices}.

Im Falle von IMVR wurde jedoch die Entscheidung getroffen, einen simplen Avatar einzubauen. Aufgrund der Art der Applikation, fiel hierbei die Wahl auf eine abstrakte Figur, die frei verfügbar und vollständig geriggt ist. Dadurch bleibt jederzeit die Möglichkeit offen, per \gls{ik} die Figur zu steuern.

Da die Hände und der Kopf nicht benötigt werden, wurden diese kurzerhand mithilfe von 3DS Max 2015 demontiert. Die vorherig erwähnte Kontrolle über IK wurde nicht implementiert, könnte aber in einem nächsten Schritt eingebaut werden.

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{bilder/avatar}
	\caption{Der verwendete Avatar mit Kopf und Armen}
	\source{http://www.turbosquid.com/FullPreview/Index.cfm/ID/833108}
	\label{fig:avatar}
\end{figure}

\section{Herausforderungen}

Während der Entwicklung kristallisierten sich zahlreiche Probleme, die gelöst werden mussten. Teilweise reichte die Zeit allerdings nicht ganz, um die Fehler zu beheben.

\subsection{Ressourcen-Management}
\label{subsec:resources}

Ein zentrales Problem bei der Erstellung von IMVR waren die Ressourcen. Besonders im Stadium, wo noch eine enorme Anzahl Bilder dargestellt werden sollten, war es wichtig, die Bilderressourcen effektiv zu verwalten.

Das Problem bei Bildtexturen ist, dass jede verwendete Textur auf die GPU gesendet werden muss und einen Draw-Call auslöst. Werden also 1000 Bilder in der Szene angezeigt, geschehen mindestens 1000 Draw-Calls, was die Performance der Applikation stark herunterzieht.

Als Lösung für dieses Problem wurde die Darstellung der Bilder in IMVR eng um den Einsatz von Atlassen konzipiert. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.4\linewidth]{diagramme/Atlas}
	\caption{Illustration der Funktionsweise eines Atlasses}
	\label{fig:atlas}
\end{figure}

Bei einem Atlas werden mehrere Texturen in eine Textur zusammengefasst und die Shader greifen dann jeweils nur auf einen Bereich der Atlas-Textur zu. In Unity ist das relativ leicht durch die \code{Sprite}-Klasse anzuwenden:

\begin{lstlisting}
Sprite.Create(texture, rect, new Vector2(0.5f, 0.5f), tileSize, 0, SpriteMeshType.FullRect);
\end{lstlisting}

Der Parameter \code{texture} referenziert hierbei die Textur, und \code{rect} beschreibt den benötigten Ausschnitt in Pixel.

\subsection{Abspielen externer Musik}
\label{subsec:musicformats}

Ein Problem von Unity ist wie gesagt die enge Anbindung eines Projektes an sein Assets-Ordner. Eine Auswirkung dieses Design-Entscheids ist, dass Unity zur Laufzeit nur unkomprimierte WAVE-Streams und Ogg Vorbis enkodierte Streams lesen kann. Diese Limitation wurde anfangs mithilfe der Bibliothek NAudio umgangen, aber da der Einsatz von NAudio, wie in Kapitel \ref{subsec:visualization} erläutert, die Benutzung von \code{GetOutputData} und \code{GetSpectrumData} verhindert, wurde dieser Ansatz verworfen.

Stattdessen wurde auf eine andere C\#-Bibliothek ausgewichen: CSCore \ref{cscore}. Im Gegenzug zu einer schlechteren Portabilität als NAudio (läuft nur auf Windows) bietet CSCore Klassen zur einfachen Konversion von codierten Audio-Streams zu simplen PCM-Streams an.

In Unity dreht sich die Audiowiedergabe um die Klassen \code{AudioListener}, \code{AudioSource} und \code{AudioClip}. Die Rolle dieser Klasse ist grösstenteils selbsterklärend: Ein \code{AudioListener} (die Kamera) hört eine Anzahl von \code{AudioSources}, welche jeweils einen \code{AudioClip} abspielen.

Im Falle von CSCore war es nötig, den \code{AudioClip} zu überschreiben. Da die Klasse jedoch \code{sealed} und somit nicht erweiterbar ist, wurde kurzerhand ein Wrapper darum erstellt. Die Wrapper-Klasse, \code{CSCAudioClip}, bietet eine Property \textit{Clip} an, welche einen gültigen \code{AudioClip} zurückliefert. 

Der Clip selbst wird direkt im Konstruktor der Klasse erstellt:

\lstinputlisting[caption={Initialisierung eines Audio-Clips per CSCore}, 
label={lst:ringvisualizer}]{code/CSCAudioClip_Initialize.cs}

Zuerst wird ein Codec-Stream erstellt, der die Datei in einen PCM-Stream umwandelt. Da Unity die Byte-Werte dieses Streams nicht interpretieren kann, müssen diese noch in Floats umgewandelt werden. Dies geschieht über einen \code{WaveToSampleBase}-Stream. Die Unterscheidung der Bit-Tiefe wird gemacht, damit der Decoder weiss, wie viele Bits zur Bildung der Floats benutzt werden müssen.

Nach diesem Prozedere kann der Clip wie ein herkömmlicher Audio-Clip einer \code{AudioSource} zugeordnet und abgespielt werden. Ein Nachteil dieser Implementation ist, dass der Clip nach Benutzung per \code{Dispose()} vom Memory gelöscht werden muss.

\subsection{Canvas-Elemente}

\subsection{Performance}
\label{subsec:performance}

%TODO: Performance der Playlist, Ladebildschirm


% Alpha-Sorting, etc. Tile-Blanket